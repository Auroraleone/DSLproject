{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eXtMuDb1lLeN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.cluster import KMeans\n",
        "import warnings\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from scipy import stats\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# DATA LOADING AND INITIAL SETUP\n",
        "# =============================================================================\n",
        "\n",
        "print(\"Loading data...\")\n",
        "# Load development (training) and evaluation (test) datasets\n",
        "df_dev = pd.read_csv(\"/content/development.csv\")    # Training data with price labels\n",
        "df_eval = pd.read_csv(\"/content/evaluation.csv\")    # Test data without prices to predict\n",
        "\n",
        "print(f\"Training data shape: {df_dev.shape}\")\n",
        "print(f\"Test data shape: {df_eval.shape}\")\n",
        "print(f\"Training data columns: {list(df_dev.columns)}\")\n",
        "print(f\"Test data columns: {list(df_eval.columns)}\")\n",
        "\n",
        "# Store original test IDs for final submission\n",
        "test_ids_for_submission = df_eval[\"id\"].copy()\n",
        "\n",
        "# Prepare datasets for concatenation\n",
        "df_eval[\"price\"] = np.nan  # Add empty price column to test set\n",
        "df_dev['is_dev'] = 1       # Flag for development/training data\n",
        "df_eval['is_dev'] = 0      # Flag for evaluation/test data\n",
        "\n",
        "# Handle missing category column in test set if needed\n",
        "if \"category\" not in df_eval.columns:\n",
        "    df_eval[\"category\"] = np.nan\n",
        "\n",
        "# Combine training and test datasets for unified preprocessing\n",
        "df = pd.concat([df_dev, df_eval], ignore_index=True, sort=False)\n",
        "print(f\"Combined dataset shape: {df.shape}\")\n",
        "print(f\"Training samples: {(df['is_dev'] == 1).sum()}\")\n",
        "print(f\"Test samples: {(df['is_dev'] == 0).sum()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1z4nwtYlbrX",
        "outputId": "53b2f193-ed72-425a-b44d-9ed5385a0f37"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Training data shape: (79589, 21)\n",
            "Test data shape: (19898, 20)\n",
            "Training data columns: ['category', 'title', 'body', 'amenities', 'bathrooms', 'bedrooms', 'currency', 'fee', 'has_photo', 'pets_allowed', 'price', 'price_type', 'square_feet', 'address', 'cityname', 'state', 'latitude', 'longitude', 'source', 'time', 'id']\n",
            "Test data columns: ['category', 'title', 'body', 'amenities', 'bathrooms', 'bedrooms', 'currency', 'fee', 'has_photo', 'pets_allowed', 'price_type', 'square_feet', 'address', 'cityname', 'state', 'latitude', 'longitude', 'source', 'time', 'id']\n",
            "Combined dataset shape: (99487, 22)\n",
            "Training samples: 79589\n",
            "Test samples: 19898\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# TARGET ENGINEERING: LOG TRANSFORMATION\n",
        "# =============================================================================\n",
        "\n",
        "print(\"Target engineering...\")\n",
        "# Store original target values for analysis\n",
        "y_train_original = df[df['is_dev'] == 1][\"price\"].copy()\n",
        "# Apply log transformation to normalize price distribution\n",
        "df.loc[df['is_dev'] == 1, 'price_log'] = np.log1p(df[df['is_dev'] == 1][\"price\"])\n",
        "\n",
        "# Compare skewness before and after transformation\n",
        "original_skew = stats.skew(y_train_original)\n",
        "log_skew = stats.skew(df[df['is_dev'] == 1]['price_log'])\n",
        "print(f\"Target skewness - Original: {original_skew:.3f}, Log-transformed: {log_skew:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDZBw_NMlg8q",
        "outputId": "18861d4e-0bb4-4af4-cd1f-9df3e908b265"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target engineering...\n",
            "Target skewness - Original: 9.129, Log-transformed: 0.436\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CONSERVATIVE OUTLIER DETECTION AND REMOVAL\n",
        "# =============================================================================\n",
        "\n",
        "print(\"Conservative outlier removal...\")\n",
        "# Use conservative approach to maintain more training data\n",
        "train_mask = df['is_dev'] == 1\n",
        "Q1 = df.loc[train_mask, 'price'].quantile(0.15)  # More conservative than Q1 (0.25)\n",
        "Q3 = df.loc[train_mask, 'price'].quantile(0.85)  # More conservative than Q3 (0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Wider bounds to remove only extreme outliers\n",
        "lower_bound = Q1 - 2.5 * IQR  # Standard is 1.5, using 2.5 for conservation\n",
        "upper_bound = Q3 + 2.5 * IQR\n",
        "\n",
        "# Identify extreme outliers\n",
        "outlier_mask = train_mask & ((df['price'] < lower_bound) | (df['price'] > upper_bound))\n",
        "outliers_removed = outlier_mask.sum()\n",
        "total_training = train_mask.sum()\n",
        "print(f\"Outliers removed: {outliers_removed} out of {total_training} ({outliers_removed/total_training*100:.1f}%)\")\n",
        "\n",
        "# Remove outliers only if they represent less than 5% of data\n",
        "if outliers_removed / total_training < 0.05:\n",
        "    df = df[~outlier_mask].reset_index(drop=True)\n",
        "    print(\"Outliers removed successfully\")\n",
        "else:\n",
        "    print(\"Too many outliers detected, keeping all data\")\n",
        "\n",
        "# =============================================================================\n",
        "# ADVANCED DATA CLEANING AND IMPUTATION\n",
        "# =============================================================================\n",
        "\n",
        "print(\"Advanced data cleaning...\")\n",
        "\n",
        "# BATHROOMS: Hierarchical imputation using related features\n",
        "df[\"bathrooms\"].fillna(\n",
        "    df.groupby(['bedrooms', 'square_feet'])['bathrooms'].transform('median'),\n",
        "    inplace=True\n",
        ")\n",
        "df[\"bathrooms\"].fillna(1, inplace=True)  # Default fallback\n",
        "\n",
        "# BEDROOMS: Similar hierarchical approach\n",
        "df[\"bedrooms\"].fillna(\n",
        "    df.groupby(['bathrooms', 'square_feet'])['bedrooms'].transform('median'),\n",
        "    inplace=True\n",
        ")\n",
        "df[\"bedrooms\"].fillna(1, inplace=True)\n",
        "\n",
        "# SQUARE FEET: Multi-level imputation strategy\n",
        "df[\"square_feet\"].fillna(\n",
        "    df.groupby(['bedrooms', 'bathrooms'])['square_feet'].transform('median'),\n",
        "    inplace=True\n",
        ")\n",
        "df[\"square_feet\"].fillna(df[\"square_feet\"].median(), inplace=True)\n",
        "\n",
        "# Conservative clipping for square feet to remove unrealistic values\n",
        "df[\"square_feet\"] = df[\"square_feet\"].clip(\n",
        "    lower=200,  # Minimum realistic apartment size\n",
        "    upper=df[\"square_feet\"].quantile(0.995)  # 99.5th percentile to preserve more data\n",
        ")\n",
        "\n",
        "# BINARY FEATURES: Clean encoding\n",
        "df[\"fee\"] = df[\"fee\"].replace({\"No\": 0, \"Yes\": 1}).fillna(0).astype(int)\n",
        "df[\"has_photo\"] = df[\"has_photo\"].astype(str).str.lower().map({\n",
        "    \"true\": 1, \"1\": 1, \"yes\": 1,\n",
        "    \"false\": 0, \"0\": 0, \"no\": 0\n",
        "}).fillna(0).astype(int)\n",
        "\n",
        "# CATEGORICAL FEATURES: Fill missing values with 'unknown'\n",
        "categorical_cols = [\"pets_allowed\", \"category\", \"cityname\", \"state\", \"price_type\"]\n",
        "for col in categorical_cols:\n",
        "    df[col].fillna(\"unknown\", inplace=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uP940YxQls46",
        "outputId": "0711218f-e205-4fc1-8714-eeb99d8c332c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conservative outlier removal...\n",
            "Outliers removed: 344 out of 79589 (0.4%)\n",
            "Outliers removed successfully\n",
            "Advanced data cleaning...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# COMPREHENSIVE FEATURE ENGINEERING\n",
        "# =============================================================================\n",
        "\n",
        "print(\"Extended feature engineering...\")\n",
        "\n",
        "# 1. ROOM-BASED FEATURES\n",
        "# Ratio of bedrooms to bathrooms (with small epsilon to avoid division by zero)\n",
        "df[\"rooms_ratio\"] = df[\"bedrooms\"] / (df[\"bathrooms\"] + 0.1)\n",
        "df[\"rooms_ratio\"] = df[\"rooms_ratio\"].clip(upper=10)  # Cap extreme ratios\n",
        "df[\"total_rooms\"] = df[\"bedrooms\"] + df[\"bathrooms\"]\n",
        "df[\"rooms_per_sqft\"] = df[\"total_rooms\"] / (df[\"square_feet\"] + 1)\n",
        "\n",
        "# 2. PRICE PER SQUARE FOOT ESTIMATES (only for training data)\n",
        "train_data = df[df['is_dev'] == 1]\n",
        "if len(train_data) > 0:\n",
        "    # Calculate average price per square foot by city\n",
        "    city_price_per_sqft = train_data.groupby('cityname')['price'].sum() / train_data.groupby('cityname')['square_feet'].sum()\n",
        "    df['city_price_per_sqft'] = df['cityname'].map(city_price_per_sqft).fillna(city_price_per_sqft.median())\n",
        "\n",
        "    # Estimate price based on city's price per square foot\n",
        "    df['estimated_price_by_city'] = df['square_feet'] * df['city_price_per_sqft']\n",
        "\n",
        "# 3. TEMPORAL FEATURES\n",
        "df['time'] = pd.to_datetime(df['time'], errors='coerce')\n",
        "df['listing_month'] = df['time'].dt.month.fillna(0).astype(int)\n",
        "df['listing_day_of_week'] = df['time'].dt.dayofweek.fillna(0).astype(int)\n",
        "df['is_weekend'] = (df['listing_day_of_week'] >= 5).astype(int)\n",
        "df['listing_hour'] = df['time'].dt.hour.fillna(12).astype(int)\n",
        "df['is_business_hours'] = ((df['listing_hour'] >= 9) & (df['listing_hour'] <= 17)).astype(int)\n",
        "\n",
        "# 4. GEOGRAPHIC FEATURES\n",
        "df['latitude'].fillna(df['latitude'].median(), inplace=True)\n",
        "df['longitude'].fillna(df['longitude'].median(), inplace=True)\n",
        "\n",
        "# Geographic clustering to capture location-based patterns\n",
        "if len(df[df['latitude'].notna() & df['longitude'].notna()]) > 100:\n",
        "    coords = df[['latitude', 'longitude']].fillna(0)\n",
        "    kmeans = KMeans(n_clusters=20, random_state=42, n_init=10)\n",
        "    df['geo_cluster'] = kmeans.fit_predict(coords)\n",
        "else:\n",
        "    df['geo_cluster'] = 0\n",
        "\n",
        "# 5. POLYNOMIAL AND INTERACTION FEATURES\n",
        "# Create polynomial features to capture non-linear relationships\n",
        "df['sqft_squared'] = df['square_feet'] ** 2\n",
        "df['total_rooms_squared'] = df['total_rooms'] ** 2\n",
        "df['bedrooms_bathrooms_interaction'] = df['bedrooms'] * df['bathrooms']\n",
        "df['sqft_rooms_interaction'] = df['square_feet'] * df['total_rooms']\n",
        "\n",
        "# 6. LOG FEATURES to improve distributions\n",
        "df['log_square_feet'] = np.log1p(df['square_feet'])\n",
        "df['log_total_rooms'] = np.log1p(df['total_rooms'])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_bFWC25lt86",
        "outputId": "c5ad0e79-1edd-44fa-aa91-bf89e5f812b1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extended feature engineering...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ADVANCED TF-IDF TEXT PROCESSING\n",
        "# =============================================================================\n",
        "\n",
        "def tfidf_features_advanced(df_input, column, max_features, ngram_range=(1,2)):\n",
        "    \"\"\"\n",
        "    Create advanced TF-IDF features from text column.\n",
        "\n",
        "    Args:\n",
        "        df_input: Input dataframe\n",
        "        column: Column name to process\n",
        "        max_features: Maximum number of features to extract\n",
        "        ngram_range: Range of n-grams to consider\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with TF-IDF features\n",
        "    \"\"\"\n",
        "    print(f\"⚙️ Advanced TF-IDF on '{column}' (max_features={max_features})\")\n",
        "\n",
        "    # Clean and preprocess text data\n",
        "    text_data = df_input[column].fillna(\"\").astype(str)\n",
        "    text_data = text_data.str.lower().str.replace(r'[^\\w\\s]', ' ', regex=True)\n",
        "\n",
        "    # Configure TF-IDF vectorizer with optimized parameters\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        max_features=max_features,\n",
        "        stop_words=\"english\",\n",
        "        ngram_range=ngram_range,\n",
        "        min_df=2,                      # Minimum document frequency\n",
        "        max_df=0.98,                   # Maximum document frequency\n",
        "        sublinear_tf=True,             # Apply sublinear scaling\n",
        "        norm='l2'                      # L2 normalization\n",
        "    )\n",
        "\n",
        "    tfidf_matrix = vectorizer.fit_transform(text_data)\n",
        "\n",
        "    return pd.DataFrame(\n",
        "        tfidf_matrix.toarray(),\n",
        "        columns=[f\"{column}_tfidf_{i}\" for i in range(tfidf_matrix.shape[1])],\n",
        "        index=df_input.index\n",
        "    )\n",
        "\n",
        "# Apply TF-IDF to text columns with different configurations\n",
        "df = pd.concat([df, tfidf_features_advanced(df, \"title\", 200, ngram_range=(1,3))], axis=1)\n",
        "df = pd.concat([df, tfidf_features_advanced(df, \"body\", 400, ngram_range=(1,2))], axis=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlvK_N-Kl1jp",
        "outputId": "988004b5-c097-4dcc-872f-f8d0310d2198"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚙️ Advanced TF-IDF on 'title' (max_features=200)\n",
            "⚙️ Advanced TF-IDF on 'body' (max_features=400)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ADVANCED AMENITIES PROCESSING\n",
        "# =============================================================================\n",
        "\n",
        "def process_amenities_advanced(amenity_series, min_count=8):\n",
        "    \"\"\"\n",
        "    Process amenities text into binary features with normalization.\n",
        "\n",
        "    Args:\n",
        "        amenity_series: Series containing amenity text\n",
        "        min_count: Minimum count for amenity to be included\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with binary amenity features\n",
        "    \"\"\"\n",
        "    print(\"⚙️ Advanced amenities processing\")\n",
        "\n",
        "    def normalize_amenity(text):\n",
        "        \"\"\"Normalize amenity names to standard categories.\"\"\"\n",
        "        if pd.isna(text):\n",
        "            return []\n",
        "\n",
        "        amenities = [a.strip().lower() for a in str(text).split(\",\") if a.strip()]\n",
        "\n",
        "        normalized = []\n",
        "        for a in amenities:\n",
        "            # Group similar amenities into standard categories\n",
        "            if 'parking' in a or 'garage' in a:\n",
        "                normalized.append('parking')\n",
        "            elif 'pet' in a or 'dog' in a or 'cat' in a:\n",
        "                normalized.append('pet_friendly')\n",
        "            elif 'laundry' in a or 'washer' in a or 'dryer' in a:\n",
        "                normalized.append('laundry')\n",
        "            elif 'gym' in a or 'fitness' in a:\n",
        "                normalized.append('gym')\n",
        "            elif 'pool' in a:\n",
        "                normalized.append('pool')\n",
        "            elif 'balcony' in a or 'patio' in a or 'deck' in a:\n",
        "                normalized.append('outdoor_space')\n",
        "            elif 'dishwasher' in a:\n",
        "                normalized.append('dishwasher')\n",
        "            elif 'hardwood' in a or 'wood' in a:\n",
        "                normalized.append('hardwood_floors')\n",
        "            else:\n",
        "                normalized.append(a)\n",
        "        return normalized\n",
        "\n",
        "    # Process all amenities and count frequencies\n",
        "    all_amenities = amenity_series.apply(normalize_amenity).explode()\n",
        "    amenity_counts = all_amenities.value_counts()\n",
        "    common_amenities = amenity_counts[amenity_counts >= min_count].index.tolist()\n",
        "\n",
        "    print(f\"Common amenities selected: {len(common_amenities)}\")\n",
        "\n",
        "    # Create binary feature matrix\n",
        "    amenity_data = pd.DataFrame(\n",
        "        0,\n",
        "        index=amenity_series.index,\n",
        "        columns=[f\"amenity_{a.replace(' ', '_').replace('-', '_')}\"\n",
        "                for a in sorted(common_amenities)]\n",
        "    )\n",
        "\n",
        "    # Fill binary features\n",
        "    for i, entry in amenity_series.items():\n",
        "        normalized_amenities = normalize_amenity(entry)\n",
        "        for a in normalized_amenities:\n",
        "            col_name = f\"amenity_{a.replace(' ', '_').replace('-', '_')}\"\n",
        "            if col_name in amenity_data.columns:\n",
        "                amenity_data.at[i, col_name] = 1\n",
        "\n",
        "    # Add total amenities count\n",
        "    amenity_data['total_amenities'] = amenity_data.sum(axis=1)\n",
        "\n",
        "    return amenity_data\n",
        "\n",
        "# Process amenities with relaxed minimum count\n",
        "df = pd.concat([df, process_amenities_advanced(df[\"amenities\"], min_count=6)], axis=1)\n",
        "\n",
        "# =============================================================================\n",
        "# DATA CLEANUP\n",
        "# =============================================================================\n",
        "\n",
        "# Remove columns no longer needed for modeling\n",
        "columns_to_drop = [\"title\", \"body\", \"amenities\", \"currency\", \"address\", \"source\", \"time\"]\n",
        "df.drop(columns=[col for col in columns_to_drop if col in df.columns],\n",
        "        inplace=True, errors=\"ignore\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lr0ddDhLl7Hr",
        "outputId": "dd3190a3-89f0-4a02-f6e6-b5b5f0df4c46"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚙️ Advanced amenities processing\n",
            "Common amenities selected: 26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# OPTIMIZED CATEGORICAL ENCODING\n",
        "# =============================================================================\n",
        "\n",
        "print(\"Optimized categorical encoding...\")\n",
        "\n",
        "def target_encode(df, cat_col, target_col, alpha=15):\n",
        "    \"\"\"\n",
        "    Apply target encoding with smoothing to categorical variables.\n",
        "\n",
        "    Args:\n",
        "        df: Input dataframe\n",
        "        cat_col: Categorical column to encode\n",
        "        target_col: Target column for encoding\n",
        "        alpha: Smoothing parameter (higher = more smoothing)\n",
        "\n",
        "    Returns:\n",
        "        Modified dataframe with target encoded column\n",
        "    \"\"\"\n",
        "    train_mask = df['is_dev'] == 1\n",
        "    if train_mask.sum() == 0:\n",
        "        return df\n",
        "\n",
        "    # Calculate global target mean\n",
        "    target_mean = df.loc[train_mask, target_col].mean()\n",
        "\n",
        "    # Calculate category-specific statistics\n",
        "    category_stats = df.loc[train_mask].groupby(cat_col)[target_col].agg(['mean', 'count'])\n",
        "\n",
        "    # Apply smoothing formula\n",
        "    smoothed_means = (\n",
        "        (category_stats['mean'] * category_stats['count'] + target_mean * alpha) /\n",
        "        (category_stats['count'] + alpha)\n",
        "    )\n",
        "\n",
        "    # Map encoded values\n",
        "    df[f'{cat_col}_target_encoded'] = df[cat_col].map(smoothed_means).fillna(target_mean)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply target encoding to high-cardinality categorical features\n",
        "high_cardinality_cols = ['cityname', 'state', 'geo_cluster']\n",
        "\n",
        "for col in high_cardinality_cols:\n",
        "    if col in df.columns:\n",
        "        df = target_encode(df, col, 'price_log', alpha=25)\n",
        "\n",
        "# Apply one-hot encoding to low-cardinality categorical features\n",
        "low_cardinality_cols = [\n",
        "    \"category\", \"pets_allowed\", \"price_type\",\n",
        "    \"listing_month\", \"listing_day_of_week\", \"listing_hour\"\n",
        "]\n",
        "\n",
        "df = pd.get_dummies(\n",
        "    df,\n",
        "    columns=[col for col in low_cardinality_cols if col in df.columns],\n",
        "    drop_first=True,  # Avoid multicollinearity\n",
        "    dummy_na=True     # Create dummy for missing values\n",
        ")\n",
        "\n",
        "# =============================================================================\n",
        "# FINAL DATA CLEANING\n",
        "# =============================================================================\n",
        "\n",
        "print(\"Final data cleaning...\")\n",
        "\n",
        "# Convert all object columns to numeric\n",
        "for col in df.columns:\n",
        "    if df[col].dtype == 'object':\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "        df[col].fillna(0, inplace=True)\n",
        "\n",
        "# Fill any remaining missing values\n",
        "df.fillna(0, inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPEKKhK3mAEJ",
        "outputId": "b100b9c6-0f42-4d27-8ff6-a2df17267e6d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized categorical encoding...\n",
            "Final data cleaning...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# FEATURE SELECTION\n",
        "# =============================================================================\n",
        "\n",
        "print(\"Feature selection...\")\n",
        "\n",
        "# Prepare training and test data\n",
        "X_train_full = df[df['is_dev'] == 1].drop(columns=[\"price\", \"price_log\", \"id\", \"is_dev\"])\n",
        "y_train_full = df[df['is_dev'] == 1][\"price_log\"]\n",
        "X_test = df[df['is_dev'] == 0].drop(columns=[\"price\", \"price_log\", \"id\", \"is_dev\"])\n",
        "\n",
        "print(f\"Features before selection: {X_train_full.shape[1]}\")\n",
        "\n",
        "# Remove low-variance features (more permissive threshold)\n",
        "variance_selector = VarianceThreshold(threshold=0.005)\n",
        "X_train_full = pd.DataFrame(\n",
        "    variance_selector.fit_transform(X_train_full),\n",
        "    columns=X_train_full.columns[variance_selector.get_support()],\n",
        "    index=X_train_full.index\n",
        ")\n",
        "X_test = pd.DataFrame(\n",
        "    variance_selector.transform(X_test),\n",
        "    columns=X_train_full.columns,\n",
        "    index=X_test.index\n",
        ")\n",
        "\n",
        "# Select top features based on correlation with target\n",
        "corr_with_target = X_train_full.corrwith(y_train_full).abs().sort_values(ascending=False)\n",
        "top_features = corr_with_target.head(800).index.tolist()\n",
        "\n",
        "X_train_full = X_train_full[top_features]\n",
        "X_test = X_test[top_features]\n",
        "\n",
        "print(f\"Features after selection: {X_train_full.shape[1]}\")\n",
        "\n",
        "# =============================================================================\n",
        "# COLUMN NAME CLEANING FOR LIGHTGBM COMPATIBILITY\n",
        "# =============================================================================\n",
        "\n",
        "def clean_column_names(df_input):\n",
        "    \"\"\"Clean column names to be compatible with LightGBM.\"\"\"\n",
        "    cols = df_input.columns\n",
        "    new_cols = []\n",
        "    for col in cols:\n",
        "        # Remove or replace problematic characters\n",
        "        new_col = col.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"<\", \"\").replace(\">\", \"\")\n",
        "        new_col = new_col.replace(\"=\", \"_eq_\").replace(\":\", \"_col_\").replace(\"/\", \"_div_\")\n",
        "        new_col = new_col.replace(\" \", \"_\").replace(\"-\", \"_\").replace(\"__\", \"_\")\n",
        "        new_col = new_col.replace(\".\", \"_dot_\").replace(\",\", \"_comma_\")\n",
        "        new_cols.append(new_col)\n",
        "    df_input.columns = new_cols\n",
        "    return df_input\n",
        "\n",
        "X_train_full = clean_column_names(X_train_full)\n",
        "X_test = clean_column_names(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuQ4hprTmuub",
        "outputId": "a0d8c7de-9397-4df9-c292-ff0055318902"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature selection...\n",
            "Features before selection: 669\n",
            "Features after selection: 111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# FINAL MODEL TRAINING - NO VALIDATION SPLIT\n",
        "# =============================================================================\n",
        "\n",
        "print(\"Training final model on ALL training data - NO VALIDATION!\")\n",
        "\n",
        "# Optimized parameters for using all training data\n",
        "final_params = {\n",
        "    'objective': 'mae',              # Mean Absolute Error objective\n",
        "    'metric': 'mae',                 # MAE metric for evaluation\n",
        "    'boosting_type': 'gbdt',         # Gradient Boosting Decision Tree\n",
        "    'num_leaves': 180,               # Number of leaves in tree\n",
        "    'learning_rate': 0.04,           # Learning rate\n",
        "    'feature_fraction': 0.8,         # Fraction of features to use\n",
        "    'bagging_fraction': 0.7,         # Fraction of data to use for bagging\n",
        "    'bagging_freq': 5,               # Frequency of bagging\n",
        "    'min_child_samples': 10,         # Minimum samples in leaf\n",
        "    'reg_alpha': 0.2,                # L1 regularization\n",
        "    'reg_lambda': 1.5,               # L2 regularization\n",
        "    'max_depth': 16,                 # Maximum tree depth\n",
        "    'n_estimators': 2500,            # Number of boosting rounds\n",
        "    'random_state': 42,              # Random seed for reproducibility\n",
        "    'n_jobs': -1,                    # Use all available cores\n",
        "    'verbosity': -1                  # Suppress verbose output\n",
        "}\n",
        "\n",
        "print(f\"Training with parameters: {final_params}\")\n",
        "print(f\"Training data shape: {X_train_full.shape}\")\n",
        "print(f\"Test data shape: {X_test.shape}\")\n",
        "print(f\"USING ALL {X_train_full.shape[0]} TRAINING SAMPLES!\")\n",
        "\n",
        "# Train model on all available training data\n",
        "model = LGBMRegressor(**final_params)\n",
        "model.fit(X_train_full, y_train_full)\n",
        "\n",
        "print(\"Training completed on ALL training data!\")\n",
        "\n",
        "# =============================================================================\n",
        "# PREDICTION AND POST-PROCESSING\n",
        "# =============================================================================\n",
        "\n",
        "print(\"Making final predictions...\")\n",
        "\n",
        "# Generate predictions on test set\n",
        "test_preds_log = model.predict(X_test)\n",
        "y_pred_final = np.expm1(test_preds_log)  # Convert back from log space\n",
        "\n",
        "print(\"No internal validation - maximum performance on test set!\")\n",
        "\n",
        "# Intelligent post-processing\n",
        "print(\"Intelligent post-processing...\")\n",
        "\n",
        "# Use all training data for clipping bounds\n",
        "all_train_prices = np.expm1(y_train_full)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLt48gaemEsT",
        "outputId": "140c9825-3ef2-4191-80ec-12fbdccca3b7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training final model on ALL training data - NO VALIDATION!\n",
            "Training with parameters: {'objective': 'mae', 'metric': 'mae', 'boosting_type': 'gbdt', 'num_leaves': 180, 'learning_rate': 0.04, 'feature_fraction': 0.8, 'bagging_fraction': 0.7, 'bagging_freq': 5, 'min_child_samples': 10, 'reg_alpha': 0.2, 'reg_lambda': 1.5, 'max_depth': 16, 'n_estimators': 2500, 'random_state': 42, 'n_jobs': -1, 'verbosity': -1}\n",
            "Training data shape: (79245, 111)\n",
            "Test data shape: (19898, 111)\n",
            "USING ALL 79245 TRAINING SAMPLES!\n",
            "Training completed on ALL training data!\n",
            "Making final predictions...\n",
            "No internal validation - maximum performance on test set!\n",
            "Intelligent post-processing...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z-fV59Vtmk0X"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SUBMISSION PREPARATION\n",
        "# =============================================================================\n",
        "\n",
        "# Create submission dataframe\n",
        "submission_df = pd.DataFrame({\n",
        "    \"Id\": test_ids_for_submission,\n",
        "    \"Predicted\": y_pred_final.astype(int)\n",
        "})\n",
        "\n",
        "# Ensure all predictions are positive\n",
        "submission_df[\"Predicted\"] = submission_df[\"Predicted\"].clip(lower=1)\n",
        "\n",
        "# Display final statistics\n",
        "print(f\"\\nFinal prediction statistics:\")\n",
        "print(f\"Min prediction: {submission_df['Predicted'].min()}\")\n",
        "print(f\"Max prediction: {submission_df['Predicted'].max()}\")\n",
        "print(f\"Mean prediction: {submission_df['Predicted'].mean():.2f}\")\n",
        "print(f\"Median prediction: {submission_df['Predicted'].median():.2f}\")\n",
        "\n",
        "print(f\"\\nTraining data price statistics:\")\n",
        "print(f\"Min price: {all_train_prices.min()}\")\n",
        "print(f\"Max price: {all_train_prices.max()}\")\n",
        "print(f\"Mean price: {all_train_prices.mean():.2f}\")\n",
        "print(f\"Median price: {all_train_prices.median():.2f}\")\n",
        "\n",
        "# Save submission file\n",
        "submission_df.to_csv(\"submission.csv\", index=False)\n",
        "print(\"\\nSubmission file created: submission.csv\")\n",
        "print(\"\\nFirst few predictions:\")\n",
        "print(submission_df.head(10))\n",
        "\n",
        "# Calculate and display feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X_train_full.columns,\n",
        "    'importance': model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(f\"\\nTop 10 most important features:\")\n",
        "print(feature_importance.head(10))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Iz65U0KmcvW",
        "outputId": "73ed1970-806a-4922-a68c-2e06fc23cce3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final prediction statistics:\n",
            "Min prediction: 371\n",
            "Max prediction: 6145\n",
            "Mean prediction: 1492.43\n",
            "Median prediction: 1352.00\n",
            "\n",
            "Training data price statistics:\n",
            "Min price: 100.00000000000003\n",
            "Max price: 5278.999999999996\n",
            "Mean price: 1494.64\n",
            "Median price: 1350.00\n",
            "\n",
            "Submission file created: submission.csv\n",
            "\n",
            "First few predictions:\n",
            "   Id  Predicted\n",
            "0   0        882\n",
            "1   1       1936\n",
            "2   2       1037\n",
            "3   3       1523\n",
            "4   4       1681\n",
            "5   5       1604\n",
            "6   6       1650\n",
            "7   7       1670\n",
            "8   8       1479\n",
            "9   9       1725\n",
            "\n",
            "Top 10 most important features:\n",
            "                    feature  importance\n",
            "0   estimated_price_by_city       31623\n",
            "17                 latitude       30942\n",
            "26                longitude       28865\n",
            "5           log_square_feet       28271\n",
            "1   cityname_target_encoded       27140\n",
            "8    sqft_rooms_interaction       24763\n",
            "2       city_price_per_sqft       24535\n",
            "43          total_amenities       14725\n",
            "56           body_tfidf_181       11681\n",
            "60           title_tfidf_49       10792\n"
          ]
        }
      ]
    }
  ]
}